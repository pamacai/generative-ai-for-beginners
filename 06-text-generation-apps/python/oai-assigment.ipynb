{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build text generation apps\n",
    "\n",
    "You've seen so far through this curriculum that there are core concepts like prompts and even a whole discipline called \"prompt engineering\". Many tools you can interact with like ChatGPT, Office 365, Microsoft Power Platform and more, support you using prompts to accomplish something.\n",
    "\n",
    "For you to add such an experience to an app, you need to understand concepts like prompts, completions and choose a library to work with. That's exactly what you'll learn in this chapter.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter, you will:\n",
    "\n",
    "- Learn about the openai library and its core concepts.\n",
    "- Build a text generation app using openai.\n",
    "- Understand how to use concepts like prompt, temperature, and tokens to build a text generation app.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "At the end of this lesson, you'll be able to:\n",
    "\n",
    "- Explain what a text generation app is.\n",
    "- Build a text generation app using openai.\n",
    "- Configure your app to use more or less tokens and also change the temperature, for a varied output.\n",
    "\n",
    "## What is a text generation app?\n",
    "\n",
    "Normally when you build an app it has some kind of interface like the following:\n",
    "\n",
    "- Command-based. Console apps are typical apps where you type a command and it carries out a task. For example, `git` is a command-based app.\n",
    "- User interface (UI). Some apps have graphical user interfaces (GUIs) where you click buttons, input text, select options and more.\n",
    "\n",
    "### Console and UI apps are limited\n",
    "\n",
    "Compare it to a command-based app where you type a command: \n",
    "\n",
    "- **It's limited**. You can't just type any command, only the ones that the app supports.\n",
    "- **Language specific**. Some apps support many languages, but by default the app is built for a specific language, even if you can add more language support. \n",
    "\n",
    "### Benefits of text generation apps\n",
    "\n",
    "So how is a text generation app different?\n",
    "\n",
    "In a text generation app, you have more flexibility, you're not limited to a set of commands or a specific input language. Instead, you can use natural language to interact with the app. Another benefit is that because you're already interacting with a data source that has been trained on a vast corpus of information, whereas a traditional app might be limited on what's in a database. \n",
    "\n",
    "### What can I build with a text generation app?\n",
    "\n",
    "There are many things you can build. For example:\n",
    "\n",
    "- **A chatbot**. A chatbot answering questions about topics, like your company and its products could be a good match.\n",
    "- **Helper**. LLMs are great at things like summarizing text, getting insights from text, producing text like resumes and more.\n",
    "- **Code assistant**. Depending on the language model you use, you can build a code assistant that helps you write code. For example, you can use a product like GitHub Copilot as well as ChatGPT to help you write code.\n",
    "\n",
    "## How can I get started?\n",
    "\n",
    "Well, you need to find a way to integrate with an LLM which usually entails the following two approaches:\n",
    "\n",
    "- Use an API. Here you're constructing web requests with your prompt and get generated text back.\n",
    "- Use a library. Libraries help encapsulate the API calls and make them easier to use.\n",
    "\n",
    "## Libraries/SDKs\n",
    "\n",
    "There are a few well known libraries for working with LLMs like:\n",
    "\n",
    "- **openai**, this library makes it easy to connect to your model and send in prompts.\n",
    "\n",
    "Then there are libraries that operate on a higher level like:\n",
    "\n",
    "- **Langchain**. Langchain is well known and supports Python.\n",
    "- **Semantic Kernel**. Semantic Kernel is a library by Microsoft supporting the languages C#, Python, and Java.\n",
    "\n",
    "## First app using openai\n",
    "\n",
    "Let's see how we can build our first app, what libraries we need, how much is required and so on.\n",
    "\n",
    "### Install openai\n",
    "\n",
    "  > [!NOTE] This step is not necessary if run this notebook on Codespaces or within a Devcontainer\n",
    "\n",
    "\n",
    "There are many libraries out there for interacting with OpenAI or Azure OpenAI. It's possible to use numerous programming languages as well like C#, Python, JavaScript, Java and more.  \n",
    "We've chosen to use the `openai` Python library, so we'll use `pip` to install it.\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "If you aren't running this notebook in a Codespaces or a Dev Container, you also need to install [Python](https://www.python.org/) on your machine.\n",
    "\n",
    "### Create a resource and locate your API key\n",
    "\n",
    "In case you didn't already, you need to carry out the following steps:\n",
    "\n",
    "- Create an account on OpenAI <https://platform.openai.com/signup>.\n",
    "- Now, get your API keys <https://platform.openai.com/api-keys>. \n",
    "\n",
    ">[!NOTE]\n",
    "> It's worth separating your API key from your code. You can do so by using environment variables.\n",
    "> - Set the environment variable `OPENAI_KEY` to your API key in your .env file. If you already completed the previous exercises of this course, you are all set up.\n",
    "> - It is important to note that the API Key will only be accessible once. Therefore, it is imperative to verify that it has been copied correctly. In the event that it does not function as intended, it is recommended to delete the key and generate a new one.\n",
    "\n",
    "\n",
    "### Setup configuration Azure\n",
    "\n",
    "If you're using Open AI, here's how you setup configuration:\n",
    "\n",
    "```python\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY']\n",
    "  )\n",
    "\n",
    "deployment=os.environ['OPENAI_DEPLOYMENT']\n",
    "```\n",
    "\n",
    "Above we're setting the following:\n",
    "\n",
    "- `api_key`, this is your API key found in THE OpenAI dashboard.\n",
    "- `deployment`, this is your GPT version.\n",
    "\n",
    "> [!NOTE]\n",
    "> `os.environ` is a function that reads environment variables. You can use it to read environment variables like `AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT`.\n",
    "\n",
    "## Generate text\n",
    "\n",
    "The way to generate text is to use the `chat.completion` class. Here's an example:\n",
    "\n",
    "```python\n",
    "prompt = \"Complete the following: Once upon a time there was a\"\n",
    "\n",
    "completion = client.chat.completions.create(model=deployment, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(completion.choices[0].message.content)\n",
    "```\n",
    "\n",
    "In the above code, we create a completion object and pass in the model we want to use and the prompt. Then we print the generated text.\n",
    "\n",
    "### Chat completions\n",
    "\n",
    "So far, you've seen how we've been using `Completion` to generate text. But there's another class called `ChatCompletion` that is more suited for chatbots. Here's an example of using it:\n",
    "\n",
    "```python\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY']\n",
    "  )\n",
    "\n",
    "deployment=os.environ['OPENAI_DEPLOYMENT']\n",
    "\n",
    "completion = client.chat.completions.create(model=deployment, messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "print(completion.choices[0].message.content)\n",
    "```\n",
    "\n",
    "More on this functionality in a coming chapter.\n",
    "\n",
    "## Exercise - your first text generation app\n",
    "\n",
    "Now that we learned how to set up and configure Azure OpenAI service, it's time to build your first text generation app. To build your app, follow these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a virtual environment and install openai:\n",
    "\n",
    "  > [!NOTE] This step is not necessary if you run this notebook on Codespaces or within a Devcontainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.55.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (2.10.1)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create virtual environment\n",
    "! python -m venv venv\n",
    "# Activate virtual environment\n",
    "! source venv/bin/activate\n",
    "# Install openai package\n",
    "! pip install openai\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> If you're using Windows type `venv\\Scripts\\activate` instead of `source venv/bin/activate`. \n",
    "\n",
    "> [!NOTE]\n",
    "> Locate your Azure Open AI key by going to https://portal.azure.com/ and search for `Open AI` and select the `Open AI resource` and then select `Keys and Endpoint` and copy the `Key 1` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a *app.py* file and give it the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]  \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m## 3. Run the prompt\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# make completion\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# completion = client.chat.completions.create(model=deployment, messages=messages)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print response\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print(completion.choices[0].message.content)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt):\n\u001b[1;32m     19\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]       \n\u001b[0;32m---> 20\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                         \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from openai import AzureOpenAI\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# client = AzureOpenAI(\n",
    "#   api_key=os.environ['OPENAI_API_KEY']\n",
    "#   )\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"Frank domain\"\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=deployment,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# add your completion code\n",
    "prompt = \"Complete the following: Once upon a time there was a\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]  \n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "\n",
    "# make completion\n",
    "# completion = client.chat.completions.create(model=deployment, messages=messages)\n",
    "\n",
    "# print response\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    You should see an output like the following:\n",
    "\n",
    "    ```output\n",
    "     very unhappy _____.\n",
    "\n",
    "    Once upon a time there was a very unhappy mermaid.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of prompts, for different things\n",
    "\n",
    "Now you've seen how to generate text using a prompt. You even have a program up and running that you can modify and change to generate different types of text. \n",
    "\n",
    "Prompts can be used for all sorts of tasks. For example:\n",
    "\n",
    "- **Generate a type of text**. For example, you can generate a poem, questions for a quiz etc.\n",
    "- **Lookup information**. You can use prompts to look for information like the following example 'What does CORS mean in web development?'.\n",
    "- **Generate code**. You can use prompts to generate code, for example developing a regular expression used to validate emails or why not generate an entire program, like a web app?  \n",
    "\n",
    "## A more practical use case: a recipe generator\n",
    "\n",
    "Imagine you have ingredients at home and you want to cook something. For that, you need a recipe. A way to find recipes is to use a search engine or you could use an LLM to do so.\n",
    "\n",
    "You could write a prompt like so:\n",
    "\n",
    "> \"Show me 5 recipes for a dish with the following ingredients: chicken, potatoes, and carrots. Per recipe, list all the ingredients used\"\n",
    "\n",
    "Given the above prompt, you might get a response similar to:\n",
    "\n",
    "```output\n",
    "1. Roasted Chicken and Vegetables: \n",
    "Ingredients: \n",
    "- 4 chicken thighs\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 2 tablespoons olive oil\n",
    "- 2 cloves garlic, minced\n",
    "- 1 teaspoon dried thyme\n",
    "- 1 teaspoon dried oregano\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "2. Chicken and Potato Stew: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 1 onion, diced\n",
    "- 2 cloves garlic, minced\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 teaspoon dried oregano\n",
    "- 1 teaspoon dried thyme\n",
    "- 1 cup chicken broth\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "3. Chicken and Potato Bake: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 onion, diced\n",
    "- 2 cloves garlic, minced\n",
    "- 1 teaspoon dried oregano\n",
    "- 1 teaspoon dried thyme\n",
    "- 1 cup chicken broth\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "4. Chicken and Potato Soup: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 1 onion, diced\n",
    "- 2 cloves garlic, minced\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 teaspoon dried oregano\n",
    "- 1 teaspoon dried thyme\n",
    "- 4 cups chicken broth\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "5. Chicken and Potato Hash: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 onion, diced\n",
    "- 2 cloves garlic, minced\n",
    "- 1 teaspoon dried oregano\n",
    "```\n",
    "\n",
    "This outcome is great, I know what to cook. At this point, what could be useful improvements are:\n",
    "\n",
    "- Filtering out ingredients I don't like or am allergic to.\n",
    "- Produce a shopping list, in case I don't have all the ingredients at home.\n",
    "\n",
    "For the above cases, let's add an additional prompt:\n",
    "\n",
    "> \"Please remove recipes with garlic as I'm allergic and replace it with something else. Also, please produce a shopping list for the recipes, considering I already have chicken, potatoes and carrots at home.\"\n",
    "\n",
    "Now you have a new result, namely:\n",
    "\n",
    "```output\n",
    "1. Roasted Chicken and Vegetables: \n",
    "Ingredients: \n",
    "- 4 chicken thighs\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 2 tablespoons olive oil\n",
    "- 1 teaspoon dried thyme\n",
    "- 1 teaspoon dried oregano\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "2. Chicken and Potato Stew: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 1 onion, diced\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 teaspoon dried oregano\n",
    "- 1 teaspoon dried thyme\n",
    "- 1 cup chicken broth\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "3. Chicken and Potato Bake: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 onion, diced\n",
    "- 1 teaspoon dried oregano\n",
    "- 1 teaspoon dried thyme\n",
    "- 1 cup chicken broth\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "4. Chicken and Potato Soup: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 1 onion, diced\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 teaspoon dried oregano\n",
    "- 1 teaspoon dried thyme\n",
    "- 4 cups chicken broth\n",
    "- Salt and pepper, to taste\n",
    "\n",
    "5. Chicken and Potato Hash: \n",
    "Ingredients: \n",
    "- 2 tablespoons olive oil\n",
    "- 2 chicken breasts, cut into cubes\n",
    "- 2 potatoes, cut into cubes\n",
    "- 2 carrots, cut into cubes\n",
    "- 1 onion, diced\n",
    "- 1 teaspoon dried oregano\n",
    "\n",
    "Shopping List: \n",
    "- Olive oil\n",
    "- Onion\n",
    "- Thyme\n",
    "- Oregano\n",
    "- Salt\n",
    "- Pepper\n",
    "```\n",
    "\n",
    "That's your five recipes, with no garlic mentioned and you also have a shopping list considering what you already have at home. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - build a recipe generator\n",
    "\n",
    "Now that we have played out a scenario, let's write code to match the demonstrated scenario. To do so, follow these steps:\n",
    "\n",
    "1. Use the existing *app.py* file as a starting point\n",
    "1. Locate the `prompt` variable and change its code to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY']\n",
    "  )\n",
    "\n",
    "deployment=os.environ['OPENAI_DEPLOYMENT']\n",
    "\n",
    "prompt = \"Show me 5 recipes for a dish with the following ingredients: chicken, potatoes, and carrots. Per recipe, list all the ingredients used\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]  \n",
    "\n",
    "# make completion\n",
    "completion = client.chat.completions.create(model=deployment, messages=messages, max_tokens=600)\n",
    "\n",
    "# print response\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now run the code, you should see an output similar to:\n",
    "\n",
    "```output\n",
    "-Chicken Stew with Potatoes and Carrots: 3 tablespoons oil, 1 onion, chopped, 2 cloves garlic, minced, 1 carrot, peeled and chopped, 1 potato, peeled and chopped, 1 bay leaf, 1 thyme sprig, 1/2 teaspoon salt, 1/4 teaspoon black pepper, 1 1/2 cups chicken broth, 1/2 cup dry white wine, 2 tablespoons chopped fresh parsley, 2 tablespoons unsalted butter, 1 1/2 pounds boneless, skinless chicken thighs, cut into 1-inch pieces\n",
    "-Oven-Roasted Chicken with Potatoes and Carrots: 3 tablespoons extra-virgin olive oil, 1 tablespoon Dijon mustard, 1 tablespoon chopped fresh rosemary, 1 tablespoon chopped fresh thyme, 4 cloves garlic, minced, 1 1/2 pounds small red potatoes, quartered, 1 1/2 pounds carrots, quartered lengthwise, 1/2 teaspoon salt, 1/4 teaspoon black pepper, 1 (4-pound) whole chicken\n",
    "-Chicken, Potato, and Carrot Casserole: cooking spray, 1 large onion, chopped, 2 cloves garlic, minced, 1 carrot, peeled and shredded, 1 potato, peeled and shredded, 1/2 teaspoon dried thyme leaves, 1/4 teaspoon salt, 1/4 teaspoon black pepper, 2 cups fat-free, low-sodium chicken broth, 1 cup frozen peas, 1/4 cup all-purpose flour, 1 cup 2% reduced-fat milk, 1/4 cup grated Parmesan cheese\n",
    "\n",
    "-One Pot Chicken and Potato Dinner: 2 tablespoons olive oil, 1 pound boneless, skinless chicken thighs, cut into 1-inch pieces, 1 large onion, chopped, 3 cloves garlic, minced, 1 carrot, peeled and chopped, 1 potato, peeled and chopped, 1 bay leaf, 1 thyme sprig, 1/2 teaspoon salt, 1/4 teaspoon black pepper, 2 cups chicken broth, 1/2 cup dry white wine\n",
    "\n",
    "-Chicken, Potato, and Carrot Curry: 1 tablespoon vegetable oil, 1 large onion, chopped, 2 cloves garlic, minced, 1 carrot, peeled and chopped, 1 potato, peeled and chopped, 1 teaspoon ground coriander, 1 teaspoon ground cumin, 1/2 teaspoon ground turmeric, 1/2 teaspoon ground ginger, 1/4 teaspoon cayenne pepper, 2 cups chicken broth, 1/2 cup dry white wine, 1 (15-ounce) can chickpeas, drained and rinsed, 1/2 cup raisins, 1/2 cup chopped fresh cilantro\n",
    "```\n",
    "\n",
    "> NOTE, your LLM is nondeterministic, so you might get different results every time you run the program.\n",
    "\n",
    "Great, let's see how we can improve things. To  improve things, we want to make sure the code is flexible, so ingredients and number of recipes can be improved and changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's change the code in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY']\n",
    "  )\n",
    "\n",
    "deployment=os.environ['OPENAI_DEPLOYMENT']\n",
    "\n",
    "no_recipes = input(\"No of recipes (for example, 5: \")\n",
    "\n",
    "ingredients = input(\"List of ingredients (for example, chicken, potatoes, and carrots: \")\n",
    "\n",
    "# interpolate the number of recipes into the prompt an ingredients\n",
    "prompt = f\"Show me {no_recipes} recipes for a dish with the following ingredients: {ingredients}. Per recipe, list all the ingredients used\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]  \n",
    "\n",
    "# make completion\n",
    "completion = client.chat.completions.create(model=deployment, messages=messages, max_tokens=600)\n",
    "\n",
    "# print response\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Taking the code for a test run, could look like this:\n",
    "    \n",
    "```output\n",
    "No of recipes (for example, 5: 3\n",
    "List of ingredients (for example, chicken, potatoes, and carrots: milk,strawberries\n",
    "\n",
    "-Strawberry milk shake: milk, strawberries, sugar, vanilla extract, ice cubes\n",
    "-Strawberry shortcake: milk, flour, baking powder, sugar, salt, unsalted butter, strawberries, whipped cream        \n",
    "-Strawberry milk: milk, strawberries, sugar, vanilla extract\n",
    "```\n",
    "\n",
    "### Improve by adding filter and shopping list\n",
    "\n",
    "We now have a working app capable of producing recipes and it's flexible as it relies on inputs from the user, both on the number of recipes but also the ingredients used.\n",
    "\n",
    "To further improve it, we want to add the following:\n",
    "\n",
    "- **Filter out ingredients**. We want to be able to filter out ingredients we don't like or are allergic to. To accomplish this change, we can edit our existing prompt and add a filter condition to the end of it like so:\n",
    "\n",
    "    ```python\n",
    "    filter = input(\"Filter (for example, vegetarian, vegan, or gluten-free: \")\n",
    "\n",
    "    prompt = f\"Show me {no_recipes} recipes for a dish with the following ingredients: {ingredients}. Per recipe, list all the ingredients used, no {filter}\"\n",
    "    ```\n",
    "\n",
    "    Above, we add `{filter}` to the end of the prompt and we also capture the filter value from the user.\n",
    "\n",
    "    An example input of running the program can now look like so:\n",
    "    \n",
    "    ```output    \n",
    "    No of recipes (for example, 5: 3\n",
    "    List of ingredients (for example, chicken, potatoes, and carrots: onion,milk\n",
    "    Filter (for example, vegetarian, vegan, or gluten-free: no milk\n",
    "\n",
    "    1. French Onion Soup\n",
    "\n",
    "    Ingredients:\n",
    "    \n",
    "    -1 large onion, sliced\n",
    "    -3 cups beef broth\n",
    "    -1 cup milk\n",
    "    -6 slices french bread\n",
    "    -1/4 cup shredded Parmesan cheese\n",
    "    -1 tablespoon butter\n",
    "    -1 teaspoon dried thyme\n",
    "    -1/4 teaspoon salt\n",
    "    -1/4 teaspoon black pepper\n",
    "    \n",
    "    Instructions:\n",
    "    \n",
    "    1. In a large pot, sauté onions in butter until golden brown.\n",
    "    2. Add beef broth, milk, thyme, salt, and pepper. Bring to a boil.\n",
    "    3. Reduce heat and simmer for 10 minutes.\n",
    "    4. Place french bread slices on soup bowls.\n",
    "    5. Ladle soup over bread.\n",
    "    6. Sprinkle with Parmesan cheese.\n",
    "    \n",
    "    2. Onion and Potato Soup\n",
    "    \n",
    "    Ingredients:\n",
    "    \n",
    "    -1 large onion, chopped\n",
    "    -2 cups potatoes, diced\n",
    "    -3 cups vegetable broth\n",
    "    -1 cup milk\n",
    "    -1/4 teaspoon black pepper\n",
    "    \n",
    "    Instructions:\n",
    "    \n",
    "    1. In a large pot, sauté onions in butter until golden brown.\n",
    "    2. Add potatoes, vegetable broth, milk, and pepper. Bring to a boil.\n",
    "    3. Reduce heat and simmer for 10 minutes.\n",
    "    4. Serve hot.\n",
    "    \n",
    "    3. Creamy Onion Soup\n",
    "    \n",
    "    Ingredients:\n",
    "    \n",
    "    -1 large onion, chopped\n",
    "    -3 cups vegetable broth\n",
    "    -1 cup milk\n",
    "    -1/4 teaspoon black pepper\n",
    "    -1/4 cup all-purpose flour\n",
    "    -1/2 cup shredded Parmesan cheese\n",
    "    \n",
    "    Instructions:\n",
    "    \n",
    "    1. In a large pot, sauté onions in butter until golden brown.\n",
    "    2. Add vegetable broth, milk, and pepper. Bring to a boil.\n",
    "    3. Reduce heat and simmer for 10 minutes.\n",
    "    4. In a small bowl, whisk together flour and Parmesan cheese until smooth.\n",
    "    5. Add to soup and simmer for an additional 5 minutes, or until soup has thickened.\n",
    "    ```\n",
    "\n",
    "    As you can see, any recipes with milk in it has been filtered out. But, if you're lactose intolerant, you might want to filter out recipes with cheese in them as well, so there's a need to be clear.\n",
    "\n",
    "    ```python\n",
    "    \n",
    "- **Produce a shopping list**. We want to produce a shopping list, considering what we already have at home.\n",
    "\n",
    "    For this functionality, we could either try to solve everything in one prompt or we could split it up into two prompts. Let's try the latter approach. Here we're suggesting adding an additional prompt, but for that to work, we need to add the result of the former prompt as context to the latter prompt. \n",
    "\n",
    "    Locate the part in the code that prints out the result from the first prompt and add the following code below:\n",
    "    \n",
    "    ```python\n",
    "    old_prompt_result = completion.choices[0].text\n",
    "    prompt = \"Produce a shopping list for the generated recipes and please don't include ingredients that I already have.\"\n",
    "    \n",
    "    new_prompt = f\"{old_prompt_result} {prompt}\"\n",
    "    messages = [{\"role\": \"user\", \"content\": new_prompt}]\n",
    "    completion = client.chat.completion.create(model=deployment, messages=messages, max_tokens=1200)\n",
    "    \n",
    "    # print response\n",
    "    print(\"Shopping list:\")\n",
    "    print(completion.choices[0].message.content)\n",
    "    ```\n",
    "\n",
    "    Note the following:\n",
    "\n",
    "    - We're constructing a new prompt by adding the result from the first prompt to the new prompt: \n",
    "    \n",
    "        ```python\n",
    "        new_prompt = f\"{old_prompt_result} {prompt}\"\n",
    "        messages = [{\"role\": \"user\", \"content\": new_prompt}]\n",
    "        ```\n",
    "\n",
    "    - We make a new request, but also considering the number of tokens we asked for in the first prompt, so this time we say `max_tokens` is 1200. \n",
    "\n",
    "        ```python\n",
    "        completion = client.chat.completion.create(model=deployment, messages=messages, max_tokens=1200)\n",
    "        ```  \n",
    "\n",
    "        Taking this code for a spin, we now arrive at the following output:\n",
    "\n",
    "        ```output\n",
    "        No of recipes (for example, 5: 2\n",
    "        List of ingredients (for example, chicken, potatoes, and carrots: apple,flour\n",
    "        Filter (for example, vegetarian, vegan, or gluten-free: sugar\n",
    "        Recipes:\n",
    "         or milk.\n",
    "        \n",
    "        -Apple and flour pancakes: 1 cup flour, 1/2 tsp baking powder, 1/2 tsp baking soda, 1/4 tsp salt, 1 tbsp sugar, 1 egg, 1 cup buttermilk or sour milk, 1/4 cup melted butter, 1 Granny Smith apple, peeled and grated\n",
    "        -Apple fritters: 1-1/2 cups flour, 1 tsp baking powder, 1/4 tsp salt, 1/4 tsp baking soda, 1/4 tsp nutmeg, 1/4 tsp cinnamon, 1/4 tsp allspice, 1/4 cup sugar, 1/4 cup vegetable shortening, 1/4 cup milk, 1 egg, 2 cups shredded, peeled apples\n",
    "        Shopping list:\n",
    "         -Flour, baking powder, baking soda, salt, sugar, egg, buttermilk, butter, apple, nutmeg, cinnamon, allspice \n",
    "        ```\n",
    "        \n",
    "- **A word on token length**. We should consider how many tokens we need to generate the text we want. Tokens cost money, so where possible, we should try to be economical with the number of tokens we use. For example, can we phrase the prompt so that we can use less tokens?\n",
    "\n",
    "   To change tokens used, you can use the `max_tokens` parameter. For example, if you want to use 100 tokens, you would do:\n",
    "\n",
    "    ```python\n",
    "    completion = client.chat.completion.create(model=deployment, messages=messages, max_tokens=100)\n",
    "    ```\n",
    "\n",
    "- **Experimenting with temperature**. Temperature is something we haven't mentioned so far but is an important context for how our program performs. The higher the temperature value the more random the output will be. Conversely the lower the temperature value the more predictable the output will be. Consider whether you want variation in your output or not.\n",
    "\n",
    "   To alter the temperature, you can use the `temperature` parameter. For example, if you want to use a temperature of 0.5, you would do:\n",
    "\n",
    "    ```python\n",
    "    completion = client.chat.completion.create(model=deployment, messages=messages, temperature=0.5)\n",
    "    ```\n",
    "\n",
    "   > Note, the closer to 1.0, the more varied the output.\n",
    "\n",
    "\n",
    "\n",
    "## Assignment\n",
    "\n",
    "For this assignment, you can choose what to build.\n",
    "\n",
    "Here are some suggestions:\n",
    "\n",
    "- Tweak the recipe generator app to improve it further. Play around with temperature values, and the prompts to see what you can come up with.\n",
    "- Build a \"study buddy\". This app should be able to answer questions about a topic for example Python, you could have prompts like \"What is a certain topic in Python?\", or you could have a prompt that says, show me code for a certain topic etc.\n",
    "- History bot, make history come alive, instruct the bot to play a certain historical character and ask it questions about its life and times. \n",
    "\n",
    "## Solution\n",
    "\n",
    "### Study buddy\n",
    "\n",
    "- \"You're an expert on the Python language\n",
    "\n",
    "    Suggest a beginner lesson for Python in the following format:\n",
    "    \n",
    "    Format:\n",
    "    - concepts:\n",
    "    - brief explanation of the lesson:\n",
    "    - exercise in code with solutions\"\n",
    "\n",
    "Above is a starter prompt, see how you can use it and tweak it to your liking.\n",
    "\n",
    "### History bot\n",
    "\n",
    "Here's some prompts you could be using:\n",
    "\n",
    "- \"You are Abe Lincoln, tell me about yourself in 3 sentences, and respond using grammar and words like Abe would have used\"\n",
    "- \"You are Abe Lincoln, respond using grammar and words like Abe would have used:\n",
    "\n",
    "   Tell me about your greatest accomplishments, in 300 words:\"\n",
    "\n",
    "## Knowledge check\n",
    "\n",
    "What does the concept temperature do?\n",
    "\n",
    "1. It controls how random the output is.\n",
    "1. It controls how big the response is.\n",
    "1. It controls how many tokens are used.\n",
    "\n",
    "A: 1\n",
    "\n",
    "What's a good way to store secrets like API keys?\n",
    "\n",
    "1. In code.\n",
    "1. In a file.\n",
    "1. In environment variables.\n",
    "\n",
    "A: 3, because environment variables are not stored in code and can be loaded from the code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
